from dash import Dash, html

app = Dash(__name__)
app.layout = html.H1("Hello Dash")

app.run(host="0.0.0.0", port=5173)


python3 -m venv venv
source venv/bin/activate

pip install -r requirements.txt

python app.py


dash
pandas
plotly
google-cloud-bigquery


# ============================================
# ✅ CECL EDA Debug Notebook (copy-paste all)
# ============================================

# Step 1: Set working directory if needed
import os
os.chdir("/path/to/cecl_eda_dashboard")  # Change this to your repo path

# Step 2: Import modules from your existing backend
from data.loader import load_data
from eda.analyzers import MissingValueAnalyzer, NumericAnalyzer

# Step 3: Load BigQuery data
df = load_data()
print("Data shape:", df.shape)
df.head()

# Step 4: Pick a variable to debug
VARIABLE = "your_variable_here"  # 👈 Replace with an actual numeric or missing-prone column name
TIME_COL = "report_date"         # 👈 Replace if your time column is different

# Step 5: Run analyzer logic (uses same logic as Dash UI)
analyzer = (
    MissingValueAnalyzer(df, VARIABLE, TIME_COL)
    if df[VARIABLE].isna().sum() > 0
    else NumericAnalyzer(df, VARIABLE, TIME_COL)
)

results = analyzer.run()

# Step 6: View Total and Current summaries
print("📊 Total Summary:")
for k, v in results["total_summary"].items():
    print(f"{k}: {v}")

print("\n📋 Current Summary:")
for k, v in results["current_summary"].items():
    print(f"{k}: {v}")

# Step 7: Plot time series result
import plotly.express as px

df_plot = results["time_series"]
fig = px.line(
    df_plot,
    x="date",
    y="value",
    color="Metric" if "Metric" in df_plot.columns else None,
    title=f"{VARIABLE} - Over Time"
)
fig.show()

import pandas as pd
from abc import ABC, abstractmethod

# -----------------------------
# Base Class
# -----------------------------
class BaseAnalyzer(ABC):
    def __init__(self, df, variable, time_col="report_date"):
        self.df = df
        self.variable = variable
        self.time_col = time_col

    @abstractmethod
    def run(self):
        pass

# -----------------------------
# Missing % Analyzer
# -----------------------------
class MissingValueAnalyzer(BaseAnalyzer):
    def run(self):
        total = self.df[self.variable].isna().mean() * 100
        current_time = self.df[self.time_col].max()
        current_df = self.df[self.df[self.time_col] == current_time]
        current = current_df[self.variable].isna().mean() * 100

        over_time = (
            self.df.set_index(self.time_col)[self.variable]
            .isna()
            .groupby(pd.Grouper(freq='M'))
            .mean()
            .mul(100)
            .reset_index(name="missing_pct")
        )

        return {
            "total_summary": {"Missing %": round(total, 2)},
            "current_summary": {"Missing %": round(current, 2)},
            "time_series": over_time.rename(columns={self.time_col: "date", "missing_pct": "value"})
        }

# -----------------------------
# Numeric Summary Analyzer
# -----------------------------
class NumericAnalyzer(BaseAnalyzer):
    def run(self):
        series = self.df[self.variable]

        total_summary = {
            "Mean": round(series.mean(), 2),
            "Median": round(series.median(), 2),
            "Std Dev": round(series.std(), 2),
            "Min": round(series.min(), 2),
            "Max": round(series.max(), 2),
        }

        current_time = self.df[self.time_col].max()
        current_series = self.df[self.df[self.time_col] == current_time][self.variable]

        current_summary = {
            "Mean": round(current_series.mean(), 2),
            "Median": round(current_series.median(), 2),
            "Std Dev": round(current_series.std(), 2),
            "Min": round(current_series.min(), 2),
            "Max": round(current_series.max(), 2),
        }

        grouped = self.df.set_index(self.time_col).groupby(pd.Grouper(freq='M'))[self.variable]
        over_time = pd.DataFrame({
            "Mean": grouped.mean(),
            "Std Dev": grouped.std(),
            "Min": grouped.min(),
            "Max": grouped.max(),
            "Median": grouped.median(),
        }).reset_index().melt(id_vars=[self.time_col], var_name="Metric", value_name="value")

        return {
            "total_summary": total_summary,
            "current_summary": current_summary,
            "time_series": over_time.rename(columns={self.time_col: "date"})
        }



# ============================
# config/settings.py
# ============================

PROJECT_ID = 'your-gcp-project-id'
DATASET = 'your_dataset_name'
TABLE = 'your_table_name'
TIME_COL = 'report_date'  # or whatever your time column is

# ============================
# data/loader.py
# ============================

from google.cloud import bigquery
import pandas as pd
from config.settings import PROJECT_ID, DATASET, TABLE

def load_data():
    client = bigquery.Client(project=PROJECT_ID)
    query = f"SELECT * FROM `{PROJECT_ID}.{DATASET}.{TABLE}`"
    df = client.query(query).to_dataframe()
    df['report_date'] = pd.to_datetime(df['report_date'])  # adjust as needed
    return df

# ============================
# eda/base_analyzer.py
# ============================

from abc import ABC, abstractmethod

class BaseAnalyzer(ABC):
    def __init__(self, df, variable, time_col="report_date"):
        self.df = df
        self.variable = variable
        self.time_col = time_col

    @abstractmethod
    def run(self):
        pass

# ============================
# eda/missing_analyzer.py
# ============================

import pandas as pd
from .base_analyzer import BaseAnalyzer

class MissingValueAnalyzer(BaseAnalyzer):
    def run(self):
        total = self.df[self.variable].isna().mean() * 100
        current_time = self.df[self.time_col].max()
        current_df = self.df[self.df[self.time_col] == current_time]
        current = current_df[self.variable].isna().mean() * 100

        over_time = (
            self.df.set_index(self.time_col)[self.variable]
            .isna()
            .groupby(pd.Grouper(freq='M'))
            .mean()
            .mul(100)
            .reset_index(name="missing_pct")
        )

        return {
            "total_summary": {"Missing %": round(total, 2)},
            "current_summary": {"Missing %": round(current, 2)},
            "time_series": over_time.rename(columns={"report_date": "date", "missing_pct": "value"})
        }

# ============================
# eda/numeric_analyzer.py
# ============================

import pandas as pd
from .base_analyzer import BaseAnalyzer

class NumericAnalyzer(BaseAnalyzer):
    def run(self):
        series = self.df[self.variable]

        total_summary = {
            "Mean": round(series.mean(), 2),
            "Median": round(series.median(), 2),
            "Std Dev": round(series.std(), 2),
            "Min": round(series.min(), 2),
            "Max": round(series.max(), 2),
        }

        current_time = self.df[self.time_col].max()
        current_series = self.df[self.df[self.time_col] == current_time][self.variable]

        current_summary = {
            "Mean": round(current_series.mean(), 2),
            "Median": round(current_series.median(), 2),
            "Std Dev": round(current_series.std(), 2),
            "Min": round(current_series.min(), 2),
            "Max": round(current_series.max(), 2),
        }

        grouped = self.df.set_index(self.time_col).groupby(pd.Grouper(freq='M'))[self.variable]
        over_time = pd.DataFrame({
            "Mean": grouped.mean(),
            "Std Dev": grouped.std(),
            "Min": grouped.min(),
            "Max": grouped.max(),
            "Median": grouped.median(),
        }).reset_index().melt(id_vars=[self.time_col], var_name="Metric", value_name="value")

        return {
            "total_summary": total_summary,
            "current_summary": current_summary,
            "time_series": over_time.rename(columns={self.time_col: "date"})
        }

# ============================
# plots/plot_factory.py
# ============================

import plotly.express as px
import dash_table

def line_plot(df, metric=None):
    if "Metric" in df.columns and metric:
        df = df[df["Metric"] == metric]
    fig = px.line(df, x="date", y="value", color="Metric" if "Metric" in df.columns else None,
                  title=f"{metric or 'Metric'} Over Time")
    return fig

def summary_table(summary_dict, title):
    rows = [{"Metric": k, "Value": v} for k, v in summary_dict.items()]
    return dash_table.DataTable(
        columns=[{"name": i, "id": i} for i in ["Metric", "Value"]],
        data=rows,
        style_table={"overflowX": "auto"},
        style_cell={"textAlign": "left"},
    )

# ============================
# ui/layout.py
# ============================

from dash import html, dcc

def build_layout(variable_options):
    return html.Div([
        html.H2("CECL EDA Dashboard"),
        dcc.Dropdown(id='variable-dropdown', options=variable_options, placeholder="Select variable"),
        html.Div(id='summary-total'),
        html.Div(id='summary-current'),
        dcc.Graph(id='plot-output')
    ])

# ============================
# ui/callbacks.py
# ============================

from dash import Output, Input
from eda.missing_analyzer import MissingValueAnalyzer
from eda.numeric_analyzer import NumericAnalyzer
from plots.plot_factory import line_plot, summary_table

def register_callbacks(app, df, numeric_vars):
    @app.callback(
        Output("summary-total", "children"),
        Output("summary-current", "children"),
        Output("plot-output", "figure"),
        Input("variable-dropdown", "value"),
    )
    def update_dashboard(variable):
        if variable is None:
            return "", "", {}

        analyzer = (
            MissingValueAnalyzer(df, variable) if df[variable].isna().sum() > 0
            else NumericAnalyzer(df, variable)
        )
        results = analyzer.run()

        total_table = summary_table(results["total_summary"], "Total Summary")
        current_table = summary_table(results["current_summary"], "Current Summary")
        plot = line_plot(results["time_series"])

        return total_table, current_table, plot

# ============================
# app.py
# ============================

from dash import Dash
from data.loader import load_data
from ui.layout import build_layout
from ui.callbacks import register_callbacks

df = load_data()
numeric_vars = df.select_dtypes(include="number").columns.tolist()

app = Dash(__name__)
app.layout = build_layout([{"label": v, "value": v} for v in numeric_vars])
register_callbacks(app, df, numeric_vars)

if __name__ == "__main__":
    app.run_server(host="0.0.0.0", port=5173, debug=True)


cecl_eda_dashboard/
├── app.py
├── data/
│   └── loader.py
├── eda/
│   ├── base_analyzer.py
│   ├── missing_analyzer.py
│   └── numeric_analyzer.py
├── plots/
│   └── plot_factory.py
├── ui/
│   ├── layout.py
│   └── callbacks.py
└── config/
    └── settings.py




WITH vintages AS (
  SELECT
    SEGMENT,
    MIN(PRD_D) AS VINTAGE_DATE
  FROM `taw-riskmodeling-prod-5207.AW_RISK_MODELING.DEPOSIT_BETA_DATA_LIVE`
  GROUP BY SEGMENT
),

joined AS (
  SELECT
    d.PRD_D,
    d.APPROACH,
    d.SEGMENT,
    d.SEGMENT_QRM,
    v.VINTAGE_DATE,
    FORMAT_DATE('%Y-%m', v.VINTAGE_DATE) AS VINTAGE,
    
    -- Months on Book calculation
    (EXTRACT(YEAR FROM d.PRD_D) - EXTRACT(YEAR FROM v.VINTAGE_DATE)) * 12 +
    (EXTRACT(MONTH FROM d.PRD_D) - EXTRACT(MONTH FROM v.VINTAGE_DATE)) AS MOB,

    d.BALANCE_TO_CALC_RATE,
    d.int_paid

  FROM `taw-riskmodeling-prod-5207.AW_RISK_MODELING.DEPOSIT_BETA_DATA_LIVE` d
  LEFT JOIN vintages v
    ON d.SEGMENT = v.SEGMENT
)

SELECT
  PRD_D,
  APPROACH,
  SEGMENT,
  SEGMENT_QRM,
  VINTAGE,
  MOB,
  SUM(BALANCE_TO_CALC_RATE) AS BALANCE,
  SUM(int_paid) AS INT_EXP,
  SAFE_DIVIDE(SUM(int_paid), SUM(BALANCE_TO_CALC_RATE)) AS IN_RATE
FROM joined
GROUP BY PRD_D, APPROACH, SEGMENT, SEGMENT_QRM, VINTAGE, MOB
ORDER BY PRD_D, APPROACH, SEGMENT, SEGMENT_QRM;




WITH base AS (
  SELECT
    PARSE_DATE('%Y-%m-%d', PRD_D) AS PRD_DATE,
    APPROACH,
    SEGMENT,
    SEGMENT_QRM,
    BALANCE_TO_CALC_RATE,
    int_paid
  FROM `taw-riskmodeling-prod-5207.AW_RISK_MODELING.DEPOSIT_BETA_DATA_LIVE`
),

vintages AS (
  SELECT
    SEGMENT,
    MIN(PARSE_DATE('%Y-%m-%d', PRD_D)) AS VINTAGE_DATE
  FROM `taw-riskmodeling-prod-5207.AW_RISK_MODELING.DEPOSIT_BETA_DATA_LIVE`
  GROUP BY SEGMENT
),

joined AS (
  SELECT
    b.PRD_DATE,
    b.APPROACH,
    b.SEGMENT,
    b.SEGMENT_QRM,
    v.VINTAGE_DATE,
    FORMAT_DATE('%Y-%m', v.VINTAGE_DATE) AS VINTAGE,
    (EXTRACT(YEAR FROM b.PRD_DATE) - EXTRACT(YEAR FROM v.VINTAGE_DATE)) * 12 +
    (EXTRACT(MONTH FROM b.PRD_DATE) - EXTRACT(MONTH FROM v.VINTAGE_DATE)) AS MOB,
    b.BALANCE_TO_CALC_RATE,
    b.int_paid
  FROM base b
  LEFT JOIN vintages v
    ON b.SEGMENT = v.SEGMENT
)

SELECT
  FORMAT_DATE('%Y-%m-%d', PRD_DATE) AS PRD_D,
  APPROACH,
  SEGMENT,
  SEGMENT_QRM,
  VINTAGE,
  MOB,
  SUM(BALANCE_TO_CALC_RATE) AS BALANCE,
  SUM(int_paid) AS INT_EXP,
  SAFE_DIVIDE(SUM(int_paid), SUM(BALANCE_TO_CALC_RATE)) AS IN_RATE
FROM joined
GROUP BY PRD_D, APPROACH, SEGMENT, SEGMENT_QRM, VINTAGE, MOB
ORDER BY PRD_D, APPROACH, SEGMENT, SEGMENT_QRM;


DATE_DIFF(DATE(PRD_D), DATE(ORGNL_PSTG_D), MONTH) AS MOB

Hi [Manager’s Name],
I hope you’re doing well! I was wondering if we could set up a quick call sometime this week. I’d really appreciate the chance to better understand your expectations for my involvement in the modeling project, and to get aligned on next steps or any upcoming plans where I can contribute.

Happy to keep it very brief — just let me know what works best for your schedule. Thanks so much

pca = PCA()
X_pca = pca.fit_transform(X_scaled)
loadings = pd.DataFrame(pca.components_.T, index=features, columns=[f'PC{i+1}' for i in range(X_scaled.shape[1])])
print(loadings)

It’s possible that commitment segmentation is more related to credit exposure than to deposit behavior.


import pandas as pd

# Ensure date column is datetime
agg_df['prd_d'] = pd.to_datetime(agg_df['prd_d'])
agg_df['year'] = agg_df['prd_d'].dt.year

# Define windows
windows = [
    (2003, 2005),
    (2006, 2009),
    (2010, 2013),
    (2014, 2017),
    (2018, 2021),
    (2022, 2024),
]

# Assign window label to each row
def assign_window(year):
    for start, end in windows:


        if start <= year <= end:
            return f"{start}-{end}"
    return None

agg_df['window'] = agg_df['year'].apply(assign_window)

# Drop records that don’t fall into any window
agg_df = agg_df[agg_df['window'].notnull()]

# Count unique clients per window
unique_clients = agg_df.groupby('window')['merged_src_ip_id'].nunique().reset_index()
unique_clients.columns = ['window', 'num_unique_clients']

print(unique_clients)


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def compare_variable_across_windows(
    df,
    variable,
    segment_col='commitments_amt',
    segment_cutoff=3_000_000,
    date_col='prd_d',
    windows=None
):
    # Default windows if not passed
    if windows is None:
        windows = {
            "2018–2021": (2018, 2021),
            "2022–2024": (2022, 2024)
        }

    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df['year'] = df[date_col].dt.year

    # Assign window labels
    df['window'] = None
    for label, (start, end) in windows.items():
        df.loc[(df['year'] >= start) & (df['year'] <= end), 'window'] = label

    # Drop nulls and filter
    df = df[df['window'].notnull() & df[variable].notnull() & df[segment_col].notnull()]

    # Define commitment segments
    df['commitment_segment'] = df[segment_col].apply(
        lambda x: f'≤{segment_cutoff//1_000_000}MM' if x <= segment_cutoff else f'>{segment_cutoff//1_000_000}MM'
    )

    # --- Plot 1: KDE by window ---
    plt.figure(figsize=(10, 6))
    sns.kdeplot(data=df, x=variable, hue='window', common_norm=False)
    plt.title(f"KDE Plot of {variable} Across Windows")
    plt.xlabel(variable)
    plt.tight_layout()
    plt.show()

    # --- Plot 2: Boxplot by window ---
    plt.figure(figsize=(8, 5))
    sns.boxplot(data=df, x='window', y=variable)
    plt.title(f"Boxplot of {variable} by Window")
    plt.ylabel(variable)
    plt.tight_layout()
    plt.show()

    # --- Plot 3: Boxplot by segment and window ---
    plt.figure(figsize=(12, 5))
    sns.boxplot(data=df, x='commitment_segment', y=variable, hue='window')
    plt.title(f"{variable} by Commitment Segment and Window")
    plt.ylabel(variable)
    plt.tight_layout()
    plt.show()

    # Segment-wise boxplot (if commitment_segment exists)
    if 'commitment_segment' in df.columns:
        plt.figure(figsize=(12, 5))
        sns.boxplot(data=df, x='commitment_segment', y=variable, hue='window')
        plt.title(f"{variable} by Segment and Window")
        plt.ylabel(variable)
        plt.tight_layout()
        plt.show()




import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Define windows
windows = {
    "2018–2021": (2018, 2021),
    "2022–2024": (2022, 2024)
}

# Add year column
deposit_df['prd_d'] = pd.to_datetime(deposit_df['prd_d'])
deposit_df['year'] = deposit_df['prd_d'].dt.year

# Combine window labels
deposit_df['window'] = None
for label, (start, end) in windows.items():
    deposit_df.loc[
        (deposit_df['year'] >= start) & (deposit_df['year'] <= end), 'window'
    ] = label

# Filter to only rows in selected windows and with non-null deb_qty
df_comp = deposit_df[
    deposit_df['window'].notnull() & deposit_df['deb_qty'].notnull()
].copy()

# KDE comparison
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df_comp, x='deb_qty', hue='window', common_norm=False)
plt.title("KDE Plot of deb_qty Across Windows")
plt.xlabel("deb_qty")
plt.show()

# Boxplot comparison
plt.figure(figsize=(8, 5))
sns.boxplot(data=df_comp, x='window', y='deb_qty')
plt.title("Boxplot of deb_qty by Window")
plt.ylabel("deb_qty")
plt.show()

# Optional: segment-wise boxplot in both windows
if 'commitment_segment' in deposit_df.columns:
    plt.figure(figsize=(12, 5))
    sns.boxplot(data=df_comp, x='commitment_segment', y='deb_qty', hue='window')
    plt.title("deb_qty by Segment and Window")
    plt.ylabel("deb_qty")
    plt.show()


import pandas as pd

# Step 1: Create two versions of df1 for the two match possibilities
df1_a = df1.copy()
df1_a['Matched_ID'] = df1_a['merged_src_ip_id']
df1_a['match_type'] = 'merged_src_ip_id'

df1_b = df1.copy()
df1_b['Matched_ID'] = df1_b['borrower_id']
df1_b['match_type'] = 'borrower_id'

# Step 2: Combine both versions
df1_combined = pd.concat([df1_a, df1_b], ignore_index=True)

# Step 3: Merge with df2 on both Matched_ID and prd_d
merged = df1_combined.merge(df2, on=['Matched_ID', 'prd_d'], how='left', suffixes=('', '_df2'))

# Step 4: Drop helper columns used for matching
merged.drop(columns=['Matched_ID', 'match_type'], inplace=True)

# Step 5: Drop duplicates (same row matched via both ids — keep first match)
merged = merged.drop_duplicates(subset=['merged_src_ip_id', 'borrower_id', 'prd_d'])

# Now this is your original df1 with columns from df2 where matches occurred
final_df1 = merged


import pandas as pd

# Example: Add a key column to match on
df1_a = df1.rename(columns={'merged_src_ip_id': 'Matched_ID'})
df1_b = df1.rename(columns={'borrower_id': 'Matched_ID'})

# Keep only necessary columns for merge
df1_a['merge_type'] = 'merged_src_ip_id'
df1_b['merge_type'] = 'borrower_id'

# Concatenate both
df1_combined = pd.concat([df1_a, df1_b])

# Merge with df2 on both Matched_ID and prd_d
merged = df1_combined.merge(df2, on=['Matched_ID', 'prd_d'], how='left')

# Remove duplicates and prioritize one match if needed
merged = merged.drop_duplicates(subset=['merged_src_ip_id', 'borrower_id', 'prd_d'])

# Drop helper columns and restore original columns
merged.drop(columns=['Matched_ID', 'merge_type'], inplace=True)

# Final df1 with matched columns from df2
final_df1 = merged



coverage_check['months_after_t0'].hist(bins=30)
print(coverage_check['months_after_t0'].describe())

coverage_check = merged.groupby('merged_src_ip_id')['months_after_t0'].max().reset_index()
coverage_check['sufficient_coverage'] = coverage_check['months_after_t0'] >= 27

# Merge this flag back into t0_df
t0_df = t0_df.merge(coverage_check[['merged_src_ip_id', 'sufficient_coverage']], on='merged_src_ip_id')
t0_df = t0_df[t0_df['sufficient_coverage']


# Unique obligors before merge
original_obligors = set(agg_df['merged_src_ip_id'].unique())

# Unique obligors in deposit data
deposit_obligors = set(deposit_df['merged_src_ip_id'].unique())

# How many obligors had deposit data
matched_obligors = original_obligors.intersection(deposit_obligors)

# Count summary
print("Original obligors:", len(original_obligors))
print("Deposit obligors:", len(deposit_obligors))
print("Matched obligors (retained):", len(matched_obligors))
print("Obligors dropped (missing in deposit):", len(original_obligors) - len(matched_obligors))


# Unique ID-date pairs before and after
original_pairs = set(zip(agg_df['merged_src_ip_id'], agg_df['prd_d']))
deposit_pairs = set(zip(deposit_df['merged_src_ip_id'], deposit_df['prd_d']))

matched_pairs = original_pairs.intersection(deposit_pairs)

print("Original ID-date pairs:", len(original_pairs))
print("Deposit ID-date pairs:", len(deposit_pairs))
print("Matched pairs (retained):", len(matched_pairs))
print("Pairs dropped:", len(original_pairs) - len(matched_pairs))




def plot_feature_analysis_with_woe_bins(X, y, model_features, plot_feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain all model_features and plot_feature_name)
    y: target variable (Series)
    model_features: list of feature names to fit the model
    plot_feature_name: the WOE-transformed feature to plot bin-wise performance for
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model using multiple features
    model = sm.GEE(y, X[model_features], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[model_features])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[plot_feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Clean WOE bin info
    bin_map_df = clean_variable_data(woe_summary_df, plot_feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Round WOE values for mapping
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == plot_feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_assignments['WoE'] = bin_assignments['WoE'].round(6)
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

    # Apply mapping
    df['Bin'] = df['Feature'].round(6).map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing bin
    df = df.dropna(subset=['Bin', 'Pretty Bin'])

    # Group and aggregate by Bin
    plot_df = df.groupby(['Bin', 'Pretty Bin']).agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Sort by numeric order of bin
    plot_df['SortKey'] = plot_df['Bin'].apply(extract_bin_lower_bound)
    plot_df = plot_df.sort_values(by='SortKey').drop(columns='SortKey')

    # Plot on single y-axis
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], color='blue', marker='o', label='Actual Target Rate')
    ax.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], color='orange', marker='x', label='Predicted Probability')

    ax.set_xlabel("Bin")
    ax.set_ylabel("Probability")
    ax.set_title(f"Target Rate vs. Bins for {plot_feature_name}")
    ax.set_xticks(range(len(plot_df)))
    ax.set_xticklabels(plot_df['Pretty Bin'], rotation=45, ha='right')
    ax.legend()
    ax.grid(True)
    plt.tight_layout()
    plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

def extract_bin_lower_bound(bin_str):
    """Helper to sort bins based on their lower bound."""
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return float('-inf') if 'inf' in bin_str.lower() else 0
    if '-inf' in bin_str.lower():
        return float('-inf')
    return float(match[0])

# --- Main Plot Function ---

def plot_feature_analysis_with_woe_bins(X, y, feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain WOE-transformed feature)
    y: target variable (Series)
    feature_name: name of the WOE-transformed feature column in X
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model to get predicted probabilities
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Clean WOE bin info
    bin_map_df = clean_variable_data(woe_summary_df, feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Round to 6 decimals for both data and dict
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_assignments['WoE'] = bin_assignments['WoE'].round(6)
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

    # Apply mapping
    df['Bin'] = df['Feature'].round(6).map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing bin
    df = df.dropna(subset=['Pretty Bin'])

    # Group and aggregate
    plot_df = df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Sort bins by numeric order of lower bound
    plot_df['SortKey'] = plot_df['Pretty Bin'].apply(extract_bin_lower_bound)
    plot_df = plot_df.sort_values(by='SortKey').drop(columns='SortKey')

    # Plot with dual y-axis
    fig, ax1 = plt.subplots(figsize=(12, 6))

    ax2 = ax1.twinx()
    ax1.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], color='blue', marker='o', label='Actual Target Rate')
    ax2.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], color='orange', marker='x', label='Predicted Probability')

    ax1.set_xlabel("Bin")
    ax1.set_ylabel("Actual Target Rate", color='blue')
    ax2.set_ylabel("Predicted Probability", color='orange')

    ax1.tick_params(axis='y', labelcolor='blue')
    ax2.tick_params(axis='y', labelcolor='orange')
    ax1.set_xticks(range(len(plot_df)))
    ax1.set_xticklabels(plot_df['Pretty Bin'], rotation=45, ha='right')

    plt.title(f"Target Rate vs. Bins for {feature_name}")
    fig.tight_layout()
    plt.grid(True)
    plt.show()




# Round WOE values in bin_dict keys to 6 decimals
bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
bin_assignments['WoE'] = bin_assignments['WoE'].round(6)  # match your data
bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

# Apply map after rounding your feature
df['Bin'] = df['Feature'].round(6).map(bin_dict)


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

# --- Main Plot Function ---

def plot_feature_analysis_with_woe_bins(X, y, feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain WOE-transformed feature)
    y: target variable (Series)
    feature_name: name of the WOE-transformed feature column in X
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model to get predicted probabilities
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Get cleaned bin mapping with pretty labels
    bin_map_df = clean_variable_data(woe_summary_df, feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Map WOE value to bin
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))
    df['Bin'] = df['Feature'].map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing mapping
    df = df.dropna(subset=['Pretty Bin'])

    # Group and aggregate
    plot_df = df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], marker='o', label='Actual Target Rate', color='blue')
    plt.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], marker='x', label='Predicted Probability', color='goldenrod')
    plt.xlabel("Bin")
    plt.ylabel("Target Rate / Predicted Probability")
    plt.title(f"Target Rate vs. Bins for {feature_name}")
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str  # fallback

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

# --- Final Plot Function ---

def plot_woe_bin_target_rate(var_name, woe_df, actual_df, predicted_probs):
    """
    - var_name: name of the variable
    - woe_df: WOE summary dataframe with bins
    - actual_df: DataFrame with actual values including WOE-transformed var_name
    - predicted_probs: predicted probabilities from model (aligned with actual_df)
    """
    # Clean bin labels
    bin_map_df = clean_variable_data(woe_df, var_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Group actual data into bins using WOE values
    actual_df = actual_df.copy()
    actual_df['Pred_Prob'] = predicted_probs

    # Get bin assignment from raw WOE value by matching the bin thresholds
    bin_assignments = woe_df[woe_df['Var'] == var_name][['Bin', 'WoE']]
    bin_assignments = bin_assignments.drop_duplicates(subset='WoE')
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))
    actual_df['Bin'] = actual_df[var_name].map(bin_dict)
    actual_df['Pretty Bin'] = actual_df['Bin'].map(bin_map)

    # Aggregate event rate and predicted prob per bin
    plot_df = actual_df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Plot
    plt.figure(figsize=(10, 5))
    plt.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], marker='o', label='Actual Target Rate', color='blue')
    plt.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], marker='x', label='Predicted Probability', color='goldenrod')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel("Bin")
    plt.ylabel("Rate / Probability")
    plt.title(f"Target Rate vs {var_name} Bins")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()


# Load WOE summary
woe_df = pd.read_csv("woe_summary.csv")

# actual_df contains the actual data and must include the WOE-transformed version of var_name
# For example: actual_df['TOTAL_OUTS'] = WOE transformed values for 'TOTAL_OUTS'

# predicted_probs = model.predict(...)

plot_woe_bin_target_rate('TOTAL_OUTS', woe_df, actual_df, predicted_probs)




def plot_feature_analysis(X, y, feature_name, nbins=10):
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import statsmodels.api as sm

    # Fit GEE model
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"], 
                   cov_struct=sm.cov_struct.Independence(), 
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Create and sort dataframe
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    }).sort_values(by='Feature')

    # Bin settings
    spacing = round(len(df) / nbins)
    stop = spacing

    actual_rates = []
    pred_probs = []
    avg_x_bin = []

    # Binning loop
    start = 0
    for i in range(nbins):
        if i == nbins - 1:
            actual_rate = df['Target'][start:].mean()
            pred_prob_bin = df['Pred_Prob'][start:].mean()
            avg_x = df['Feature'][start:].mean()
        else:
            actual_rate = df['Target'][start:stop].mean()
            pred_prob_bin = df['Pred_Prob'][start:stop].mean()
            avg_x = df['Feature'][start:stop].mean()

        actual_rates.append(actual_rate)
        pred_probs.append(pred_prob_bin)
        avg_x_bin.append(avg_x)

        start = stop
        stop += spacing

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(avg_x_bin, actual_rates, marker='o', label='Actual Target Rate', color='blue')
    plt.plot(avg_x_bin, pred_probs, marker='x', label='Predicted Probability', color='goldenrod')
    plt.xlabel(feature_name)
    plt.ylabel('Target Rate / Predicted Probability')
    plt.title(f'Target Rate vs. {feature_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



Cramér’s V: Effect Size for Default Rate Differences

While the Chi-squared test determines whether a statistically significant relationship exists between segment membership (≤3MM vs >3MM) and default outcomes, it does not indicate the strength of that relationship. In large datasets, even small differences in default rates can produce significant p-values — especially when comparing segmentations with different numbers of groups (e.g., 2 vs 3), which results in different degrees of freedom. This makes raw test statistics not directly comparable between the proposed and legacy segmentation schemes.

To address this, we calculate Cramér’s V, a normalized measure of association strength for categorical variables. It ranges from 0 (no association) to 1 (perfect association), and is defined as:

V = \sqrt{\frac{\chi^2}{n \cdot (k - 1)}}

Where:
	•	\chi^2 = Chi-squared test statistic
	•	n = total number of observations
	•	k = the smaller of number of rows or columns in the contingency table

Interpretation Guidelines
	•	V < 0.10: Weak association
	•	0.10 \leq V < 0.30: Moderate association
	•	V \geq 0.30: Strong association



There is no statistically significant difference in default rates between the ≤3MM and >3MM commitment segments.

χ² = ∑ ((O_ij - E_ij)^2) / E_ij


## 4. Framework: Rolling Window Design

### Purpose

To ensure statistical validity and avoid overstating significance, we apply all segmentation tests using a **rolling window methodology**. This framework addresses the panel nature of the data and aligns analysis with changing macroeconomic cycles.

### Panel Structure and Motivation

The source data is panel-structured: each obligor has multiple time-indexed records. Conducting statistical tests on this directly would:
- Violate the assumption of independent observations
- Inflate sample size by duplicating obligors across time
- Introduce bias from events (e.g., default) that may occur long after the initial risk profile

### Design of the Rolling Window Framework

2.2 Design of the Rolling Window Framework

To ensure statistical rigor while maintaining temporal relevance, we implemented a rolling window framework. This approach addresses the panel nature of the data and avoids inflation of sample size by collapsing each obligor’s history into a single, time-aligned record per period.

Before finalizing the windows, we conducted an exploratory analysis of new default activity using a histogram of obligors newly defaulting each year. This allowed us to identify macroeconomic inflection points — such as spikes around the 2008 financial crisis or COVID-era instability — and group years into windows that align with distinct economic cycles.

We defined the following six rolling windows, each spanning 4 years:
	•	2003–2005: Pre-GFC expansion
	•	2006–2009: Global Financial Crisis
	•	2010–2013: Post-GFC recovery
	•	2014–2017: Stable low-rate environment
	•	2018–2021: Pandemic and stimulus period
	•	2022–2024: Inflationary tightening and normalization

Within each window:
	•	We extract the first observed record (T₀) per obligor
	•	We assess feature values at T₀ (e.g., risk rating, commitment, outstandings)
	•	We track default behavior within 12 months following T₀

This framework enables:
	•	Consistent alignment of features and outcomes
	•	One-time sampling per obligor per window
	•	Comparability of results across windows

By grounding the windows in empirical default behavior and aligning them with macroeconomic phases, this structure ensures that segmentation performance is evaluated not just in aggregate, but across varying credit environments.



We divide the full dataset (2003–2024) into consecutive **rolling windows**, each spanning four years:
- 2003–2005
- 2006–2009
- 2010–2013
- 2014–2017
- 2018–2021
- 2022–2024

These were chosen to **capture full economic cycles**, including pre-crisis, crisis, recovery, and post-pandemic periods.

Within each window:
- For each obligor, we identify the **first active snapshot** (T₀)
- Use that T₀ record to extract credit features
- Track whether the obligor **defaulted in the next 12 months** following T₀

This ensures:
- One record per obligor per window
- Alignment of features and outcomes
- Testing is repeated across different market conditions for stability

---

## 5. Method 1: Chi-Squared Test on Default Rates

### Objective

To test whether **default rates differ significantly** between segments (≤3MM vs >3MM), across time windows.

### Methodology

For each rolling window, we build a **2x2 contingency table** of default outcomes:

| Segment | Default (1) | No Default (0) |
|---------|-------------|----------------|
| ≤ 3MM   |      a      |       b        |
| > 3MM   |      c      |       d        |

We apply a **Chi-squared test of independence**:



χ² = ∑ ((Oᵢⱼ - Eᵢⱼ)² / Eᵢⱼ)

Where:
- *Oᵢⱼ* is the observed frequency
- *Eᵢⱼ* is the expected frequency under the assumption of independence

### Hypotheses

- **H₀ (Null):** Default rates are independent of commitment segment  
- **H₁ (Alt):** Default rates differ between commitment segments

### Pros
- Simple and interpretable
- Applicable to binary outcomes
- Useful for comparing performance across rolling windows

### Cons
- Sensitive to low cell counts (sparse defaults)
- Does not explain *why* default differs
- Requires binary outcome; no insight into underlying drivers

### [Insert Results Here]

---

## 6. Method 2: Mann–Whitney U Test on Credit Characteristics

### Objective

To test whether the **distributions of key credit features** differ between commitment segments — indicating structural differences in credit risk profiles.

### Variables Analyzed

- **Risk Rating:** Ordinal credit risk classification  
- **Outstanding Amount:** Continuous exposure measure

These variables reflect key risk characteristics available at origination and distinguish obligor financial scale and quality.

### Methodology

The **Mann–Whitney U Test** compares whether one group tends to have higher values than the other — without assuming normality.

U = min(U₁, U₂), where U₁ = R₁ - n₁(n₁ + 1) / 2

### Hypotheses (for each variable):

- **H₀:** Distribution of variable is the same across segments  
- **H₁:** Distributions differ significantly between segments

### Pros
- Non-parametric: handles skewed variables (e.g., outstandings)
- Appropriate for ordinal and continuous features
- Highlights structural credit differences

### Cons
- Only applicable to 2-group segmentation (not legacy)
- Sensitive to sample imbalance or ties
- Does not indicate effect size (though Cliff’s delta could be used)

### [Insert Results Here]

---

## 7. Method 3: PCA Decomposition for Structural Validation

### Objective

To visually and analytically assess whether commitment segments naturally form **distinct clusters** in multivariate credit space.

### Methodology

We apply **Principal Component Analysis (PCA)** to a set of scaled credit features:
- Risk Rating
- Outstanding Amount
- Commitment Amount
- (Optional: Interest Rate)

All features are standardized as:

Z = (x - μ) / σ

PCA projects the data into two orthogonal axes (PC1, PC2) that explain the **maximum variance**. We then visualize obligors in PC1–PC2 space, colored by:
- Commitment Segment
- Modeling Segment (for comparison)

### Interpretation

- **If commitment segments cluster separately**, it suggests:
  - Credit profiles differ structurally
  - Commitment segmentation aligns with latent credit behavior

- **If legacy segments show overlap**, it supports moving to a simpler, risk-aligned segmentation.

### Pros
- Reveals underlying structure of credit features
- Unsupervised (not dependent on outcome)
- Enables side-by-side visual comparison of segmentation schemes

### Cons
- PCA is linear and sensitive to outliers
- Visual inspection is subjective unless supplemented with metrics
- Segment overlap may reflect model limitations or true similarity

### [Insert Results Here]








Background and Motivation

The Commercial and Industrial (C&I) portfolio at [Your Bank Name] has traditionally been segmented into separate modeling populations based on business groupings such as Middle Market, Institutional Banking, and Leasing. Each of these segments historically had its own Probability of Default (PD) model, designed to reflect presumed differences in credit risk behavior across business lines.

However, this legacy segmentation strategy, while organizationally intuitive, has notable drawbacks:
	•	Segment boundaries are operational, not risk-based, and may group together obligors with heterogeneous risk profiles.
	•	Certain segments have become increasingly thin, especially in volatile periods, weakening model stability and leading to challenges in performance tracking and model governance.
	•	As obligor behavior shifts with market cycles and underwriting changes, static business-line definitions fail to reflect evolving credit dynamics.

⸻

Motivation for Exploring Commitment-Based Segmentation

To address these limitations, we evaluated an alternative approach: segmenting obligors based on their total commitment amount, using a cutoff of $3MM to define:
	•	≤ $3MM: Typically smaller businesses with more limited credit access, often exposed to localized economic trends
	•	> $3MM: Larger, more sophisticated firms with broader access to capital markets and more robust financial disclosures

This segmentation strategy is appealing for several reasons:
	•	Commitment is a consistent, objective, and readily available measure across all obligors.
	•	It introduces parsimonious segmentation (2 groups instead of 3–4), reducing model governance complexity.
	•	From a credit risk perspective, commitment size naturally proxies for borrower scale, financial health, and resource availability.
	•	Operationally, commitment-based segmentation aligns better with certain regulatory expectations for threshold-based risk differentiation.

⸻

Regulatory and Model Risk Considerations

Any change in segmentation methodology must be justified through a statistically sound framework that demonstrates:
	•	The new segmentation produces meaningfully different risk profiles
	•	It captures structural differences in credit behavior
	•	It performs at least as well as the legacy approach across economic cycles

To that end, we developed a three-phase statistical testing methodology applied across rolling economic windows, which is described in the following sections

Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature




Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature
