## 4. Framework: Rolling Window Design

### Purpose

To ensure statistical validity and avoid overstating significance, we apply all segmentation tests using a **rolling window methodology**. This framework addresses the panel nature of the data and aligns analysis with changing macroeconomic cycles.

### Panel Structure and Motivation

The source data is panel-structured: each obligor has multiple time-indexed records. Conducting statistical tests on this directly would:
- Violate the assumption of independent observations
- Inflate sample size by duplicating obligors across time
- Introduce bias from events (e.g., default) that may occur long after the initial risk profile

### Design of the Rolling Window Framework

2.2 Design of the Rolling Window Framework

To ensure statistical rigor while maintaining temporal relevance, we implemented a rolling window framework. This approach addresses the panel nature of the data and avoids inflation of sample size by collapsing each obligor’s history into a single, time-aligned record per period.

Before finalizing the windows, we conducted an exploratory analysis of new default activity using a histogram of obligors newly defaulting each year. This allowed us to identify macroeconomic inflection points — such as spikes around the 2008 financial crisis or COVID-era instability — and group years into windows that align with distinct economic cycles.

We defined the following six rolling windows, each spanning 4 years:
	•	2003–2005: Pre-GFC expansion
	•	2006–2009: Global Financial Crisis
	•	2010–2013: Post-GFC recovery
	•	2014–2017: Stable low-rate environment
	•	2018–2021: Pandemic and stimulus period
	•	2022–2024: Inflationary tightening and normalization

Within each window:
	•	We extract the first observed record (T₀) per obligor
	•	We assess feature values at T₀ (e.g., risk rating, commitment, outstandings)
	•	We track default behavior within 12 months following T₀

This framework enables:
	•	Consistent alignment of features and outcomes
	•	One-time sampling per obligor per window
	•	Comparability of results across windows

By grounding the windows in empirical default behavior and aligning them with macroeconomic phases, this structure ensures that segmentation performance is evaluated not just in aggregate, but across varying credit environments.



We divide the full dataset (2003–2024) into consecutive **rolling windows**, each spanning four years:
- 2003–2005
- 2006–2009
- 2010–2013
- 2014–2017
- 2018–2021
- 2022–2024

These were chosen to **capture full economic cycles**, including pre-crisis, crisis, recovery, and post-pandemic periods.

Within each window:
- For each obligor, we identify the **first active snapshot** (T₀)
- Use that T₀ record to extract credit features
- Track whether the obligor **defaulted in the next 12 months** following T₀

This ensures:
- One record per obligor per window
- Alignment of features and outcomes
- Testing is repeated across different market conditions for stability

---

## 5. Method 1: Chi-Squared Test on Default Rates

### Objective

To test whether **default rates differ significantly** between segments (≤3MM vs >3MM), across time windows.

### Methodology

For each rolling window, we build a **2x2 contingency table** of default outcomes:

| Segment | Default (1) | No Default (0) |
|---------|-------------|----------------|
| ≤ 3MM   |      a      |       b        |
| > 3MM   |      c      |       d        |

We apply a **Chi-squared test of independence**:



χ² = ∑ ((Oᵢⱼ - Eᵢⱼ)² / Eᵢⱼ)

Where:
- *Oᵢⱼ* is the observed frequency
- *Eᵢⱼ* is the expected frequency under the assumption of independence

### Hypotheses

- **H₀ (Null):** Default rates are independent of commitment segment  
- **H₁ (Alt):** Default rates differ between commitment segments

### Pros
- Simple and interpretable
- Applicable to binary outcomes
- Useful for comparing performance across rolling windows

### Cons
- Sensitive to low cell counts (sparse defaults)
- Does not explain *why* default differs
- Requires binary outcome; no insight into underlying drivers

### [Insert Results Here]

---

## 6. Method 2: Mann–Whitney U Test on Credit Characteristics

### Objective

To test whether the **distributions of key credit features** differ between commitment segments — indicating structural differences in credit risk profiles.

### Variables Analyzed

- **Risk Rating:** Ordinal credit risk classification  
- **Outstanding Amount:** Continuous exposure measure

These variables reflect key risk characteristics available at origination and distinguish obligor financial scale and quality.

### Methodology

The **Mann–Whitney U Test** compares whether one group tends to have higher values than the other — without assuming normality.

U = min(U₁, U₂), where U₁ = R₁ - n₁(n₁ + 1) / 2

### Hypotheses (for each variable):

- **H₀:** Distribution of variable is the same across segments  
- **H₁:** Distributions differ significantly between segments

### Pros
- Non-parametric: handles skewed variables (e.g., outstandings)
- Appropriate for ordinal and continuous features
- Highlights structural credit differences

### Cons
- Only applicable to 2-group segmentation (not legacy)
- Sensitive to sample imbalance or ties
- Does not indicate effect size (though Cliff’s delta could be used)

### [Insert Results Here]

---

## 7. Method 3: PCA Decomposition for Structural Validation

### Objective

To visually and analytically assess whether commitment segments naturally form **distinct clusters** in multivariate credit space.

### Methodology

We apply **Principal Component Analysis (PCA)** to a set of scaled credit features:
- Risk Rating
- Outstanding Amount
- Commitment Amount
- (Optional: Interest Rate)

All features are standardized as:

Z = (x - μ) / σ

PCA projects the data into two orthogonal axes (PC1, PC2) that explain the **maximum variance**. We then visualize obligors in PC1–PC2 space, colored by:
- Commitment Segment
- Modeling Segment (for comparison)

### Interpretation

- **If commitment segments cluster separately**, it suggests:
  - Credit profiles differ structurally
  - Commitment segmentation aligns with latent credit behavior

- **If legacy segments show overlap**, it supports moving to a simpler, risk-aligned segmentation.

### Pros
- Reveals underlying structure of credit features
- Unsupervised (not dependent on outcome)
- Enables side-by-side visual comparison of segmentation schemes

### Cons
- PCA is linear and sensitive to outliers
- Visual inspection is subjective unless supplemented with metrics
- Segment overlap may reflect model limitations or true similarity

### [Insert Results Here]








Background and Motivation

The Commercial and Industrial (C&I) portfolio at [Your Bank Name] has traditionally been segmented into separate modeling populations based on business groupings such as Middle Market, Institutional Banking, and Leasing. Each of these segments historically had its own Probability of Default (PD) model, designed to reflect presumed differences in credit risk behavior across business lines.

However, this legacy segmentation strategy, while organizationally intuitive, has notable drawbacks:
	•	Segment boundaries are operational, not risk-based, and may group together obligors with heterogeneous risk profiles.
	•	Certain segments have become increasingly thin, especially in volatile periods, weakening model stability and leading to challenges in performance tracking and model governance.
	•	As obligor behavior shifts with market cycles and underwriting changes, static business-line definitions fail to reflect evolving credit dynamics.

⸻

Motivation for Exploring Commitment-Based Segmentation

To address these limitations, we evaluated an alternative approach: segmenting obligors based on their total commitment amount, using a cutoff of $3MM to define:
	•	≤ $3MM: Typically smaller businesses with more limited credit access, often exposed to localized economic trends
	•	> $3MM: Larger, more sophisticated firms with broader access to capital markets and more robust financial disclosures

This segmentation strategy is appealing for several reasons:
	•	Commitment is a consistent, objective, and readily available measure across all obligors.
	•	It introduces parsimonious segmentation (2 groups instead of 3–4), reducing model governance complexity.
	•	From a credit risk perspective, commitment size naturally proxies for borrower scale, financial health, and resource availability.
	•	Operationally, commitment-based segmentation aligns better with certain regulatory expectations for threshold-based risk differentiation.

⸻

Regulatory and Model Risk Considerations

Any change in segmentation methodology must be justified through a statistically sound framework that demonstrates:
	•	The new segmentation produces meaningfully different risk profiles
	•	It captures structural differences in credit behavior
	•	It performs at least as well as the legacy approach across economic cycles

To that end, we developed a three-phase statistical testing methodology applied across rolling economic windows, which is described in the following sections

Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature




Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature
