
coverage_check = merged.groupby('merged_src_ip_id')['months_after_t0'].max().reset_index()
coverage_check['sufficient_coverage'] = coverage_check['months_after_t0'] >= 27

# Merge this flag back into t0_df
t0_df = t0_df.merge(coverage_check[['merged_src_ip_id', 'sufficient_coverage']], on='merged_src_ip_id')
t0_df = t0_df[t0_df['sufficient_coverage']


# Unique obligors before merge
original_obligors = set(agg_df['merged_src_ip_id'].unique())

# Unique obligors in deposit data
deposit_obligors = set(deposit_df['merged_src_ip_id'].unique())

# How many obligors had deposit data
matched_obligors = original_obligors.intersection(deposit_obligors)

# Count summary
print("Original obligors:", len(original_obligors))
print("Deposit obligors:", len(deposit_obligors))
print("Matched obligors (retained):", len(matched_obligors))
print("Obligors dropped (missing in deposit):", len(original_obligors) - len(matched_obligors))


# Unique ID-date pairs before and after
original_pairs = set(zip(agg_df['merged_src_ip_id'], agg_df['prd_d']))
deposit_pairs = set(zip(deposit_df['merged_src_ip_id'], deposit_df['prd_d']))

matched_pairs = original_pairs.intersection(deposit_pairs)

print("Original ID-date pairs:", len(original_pairs))
print("Deposit ID-date pairs:", len(deposit_pairs))
print("Matched pairs (retained):", len(matched_pairs))
print("Pairs dropped:", len(original_pairs) - len(matched_pairs))




def plot_feature_analysis_with_woe_bins(X, y, model_features, plot_feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain all model_features and plot_feature_name)
    y: target variable (Series)
    model_features: list of feature names to fit the model
    plot_feature_name: the WOE-transformed feature to plot bin-wise performance for
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model using multiple features
    model = sm.GEE(y, X[model_features], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[model_features])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[plot_feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Clean WOE bin info
    bin_map_df = clean_variable_data(woe_summary_df, plot_feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Round WOE values for mapping
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == plot_feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_assignments['WoE'] = bin_assignments['WoE'].round(6)
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

    # Apply mapping
    df['Bin'] = df['Feature'].round(6).map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing bin
    df = df.dropna(subset=['Bin', 'Pretty Bin'])

    # Group and aggregate by Bin
    plot_df = df.groupby(['Bin', 'Pretty Bin']).agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Sort by numeric order of bin
    plot_df['SortKey'] = plot_df['Bin'].apply(extract_bin_lower_bound)
    plot_df = plot_df.sort_values(by='SortKey').drop(columns='SortKey')

    # Plot on single y-axis
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], color='blue', marker='o', label='Actual Target Rate')
    ax.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], color='orange', marker='x', label='Predicted Probability')

    ax.set_xlabel("Bin")
    ax.set_ylabel("Probability")
    ax.set_title(f"Target Rate vs. Bins for {plot_feature_name}")
    ax.set_xticks(range(len(plot_df)))
    ax.set_xticklabels(plot_df['Pretty Bin'], rotation=45, ha='right')
    ax.legend()
    ax.grid(True)
    plt.tight_layout()
    plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

def extract_bin_lower_bound(bin_str):
    """Helper to sort bins based on their lower bound."""
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return float('-inf') if 'inf' in bin_str.lower() else 0
    if '-inf' in bin_str.lower():
        return float('-inf')
    return float(match[0])

# --- Main Plot Function ---

def plot_feature_analysis_with_woe_bins(X, y, feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain WOE-transformed feature)
    y: target variable (Series)
    feature_name: name of the WOE-transformed feature column in X
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model to get predicted probabilities
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Clean WOE bin info
    bin_map_df = clean_variable_data(woe_summary_df, feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Round to 6 decimals for both data and dict
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_assignments['WoE'] = bin_assignments['WoE'].round(6)
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

    # Apply mapping
    df['Bin'] = df['Feature'].round(6).map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing bin
    df = df.dropna(subset=['Pretty Bin'])

    # Group and aggregate
    plot_df = df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Sort bins by numeric order of lower bound
    plot_df['SortKey'] = plot_df['Pretty Bin'].apply(extract_bin_lower_bound)
    plot_df = plot_df.sort_values(by='SortKey').drop(columns='SortKey')

    # Plot with dual y-axis
    fig, ax1 = plt.subplots(figsize=(12, 6))

    ax2 = ax1.twinx()
    ax1.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], color='blue', marker='o', label='Actual Target Rate')
    ax2.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], color='orange', marker='x', label='Predicted Probability')

    ax1.set_xlabel("Bin")
    ax1.set_ylabel("Actual Target Rate", color='blue')
    ax2.set_ylabel("Predicted Probability", color='orange')

    ax1.tick_params(axis='y', labelcolor='blue')
    ax2.tick_params(axis='y', labelcolor='orange')
    ax1.set_xticks(range(len(plot_df)))
    ax1.set_xticklabels(plot_df['Pretty Bin'], rotation=45, ha='right')

    plt.title(f"Target Rate vs. Bins for {feature_name}")
    fig.tight_layout()
    plt.grid(True)
    plt.show()




# Round WOE values in bin_dict keys to 6 decimals
bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
bin_assignments['WoE'] = bin_assignments['WoE'].round(6)  # match your data
bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))

# Apply map after rounding your feature
df['Bin'] = df['Feature'].round(6).map(bin_dict)


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

# --- Main Plot Function ---

def plot_feature_analysis_with_woe_bins(X, y, feature_name, woe_summary_df):
    """
    X: input DataFrame (must contain WOE-transformed feature)
    y: target variable (Series)
    feature_name: name of the WOE-transformed feature column in X
    woe_summary_df: dataframe loaded from 'woe_summary.csv'
    """

    # Fit GEE model to get predicted probabilities
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"],
                   cov_struct=sm.cov_struct.Independence(),
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Prepare working DataFrame
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    })

    # Get cleaned bin mapping with pretty labels
    bin_map_df = clean_variable_data(woe_summary_df, feature_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Map WOE value to bin
    bin_assignments = woe_summary_df[woe_summary_df['Var'] == feature_name][['Bin', 'WoE']].drop_duplicates(subset='WoE')
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))
    df['Bin'] = df['Feature'].map(bin_dict)
    df['Pretty Bin'] = df['Bin'].map(bin_map)

    # Drop rows with missing mapping
    df = df.dropna(subset=['Pretty Bin'])

    # Group and aggregate
    plot_df = df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], marker='o', label='Actual Target Rate', color='blue')
    plt.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], marker='x', label='Predicted Probability', color='goldenrod')
    plt.xlabel("Bin")
    plt.ylabel("Target Rate / Predicted Probability")
    plt.title(f"Target Rate vs. Bins for {feature_name}")
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import re

# --- Helper Functions ---

def format_bin_label(bin_str):
    match = re.findall(r"[-+]?\d*\.\d+|\d+", bin_str)
    if not match:
        return bin_str
    nums = [float(m) for m in match]
    left_raw = bin_str.strip('()[]').split(',')[0]
    right_raw = bin_str.strip('()[]').split(',')[1] if ',' in bin_str else ''
    left_inf = '-inf' in left_raw.lower()
    right_inf = 'inf' in right_raw.lower()

    def smart_fmt(x):
        if x >= 1000:
            return f"{round(x / 1000)}k"
        elif x.is_integer():
            return str(int(x))
        else:
            return f"{round(x, 2)}"

    if left_inf and not right_inf:
        return f"≤ {smart_fmt(nums[0])}"
    elif right_inf and not left_inf:
        return f"{smart_fmt(nums[0])}+"
    elif not left_inf and not right_inf:
        return f"{smart_fmt(nums[0])} - {smart_fmt(nums[1])}"
    else:
        return bin_str  # fallback

def clean_variable_data(df, var_name):
    var_df = df[df['Var'] == var_name].copy()
    var_df['Bin'] = var_df['Bin'].astype(str).str.strip().str.lower()
    var_df['Bin'] = var_df['Bin'].replace({'special': 'Before 2020', 'missing': 'No Relationship'})
    var_df = var_df[~((var_df['Bin'] == 'no relationship') & (var_df['Event rate'] == 0))]
    var_df['Pretty Bin'] = var_df['Bin'].apply(lambda x: x if x in ['no relationship', 'before 2020'] else format_bin_label(x))
    return var_df

# --- Final Plot Function ---

def plot_woe_bin_target_rate(var_name, woe_df, actual_df, predicted_probs):
    """
    - var_name: name of the variable
    - woe_df: WOE summary dataframe with bins
    - actual_df: DataFrame with actual values including WOE-transformed var_name
    - predicted_probs: predicted probabilities from model (aligned with actual_df)
    """
    # Clean bin labels
    bin_map_df = clean_variable_data(woe_df, var_name)[['Bin', 'Pretty Bin']]
    bin_map = dict(zip(bin_map_df['Bin'], bin_map_df['Pretty Bin']))

    # Group actual data into bins using WOE values
    actual_df = actual_df.copy()
    actual_df['Pred_Prob'] = predicted_probs

    # Get bin assignment from raw WOE value by matching the bin thresholds
    bin_assignments = woe_df[woe_df['Var'] == var_name][['Bin', 'WoE']]
    bin_assignments = bin_assignments.drop_duplicates(subset='WoE')
    bin_dict = dict(zip(bin_assignments['WoE'], bin_assignments['Bin']))
    actual_df['Bin'] = actual_df[var_name].map(bin_dict)
    actual_df['Pretty Bin'] = actual_df['Bin'].map(bin_map)

    # Aggregate event rate and predicted prob per bin
    plot_df = actual_df.groupby('Pretty Bin').agg(
        Actual_Target_Rate=('Target', 'mean'),
        Predicted_Prob=('Pred_Prob', 'mean')
    ).reset_index()

    # Plot
    plt.figure(figsize=(10, 5))
    plt.plot(plot_df['Pretty Bin'], plot_df['Actual_Target_Rate'], marker='o', label='Actual Target Rate', color='blue')
    plt.plot(plot_df['Pretty Bin'], plot_df['Predicted_Prob'], marker='x', label='Predicted Probability', color='goldenrod')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel("Bin")
    plt.ylabel("Rate / Probability")
    plt.title(f"Target Rate vs {var_name} Bins")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()


# Load WOE summary
woe_df = pd.read_csv("woe_summary.csv")

# actual_df contains the actual data and must include the WOE-transformed version of var_name
# For example: actual_df['TOTAL_OUTS'] = WOE transformed values for 'TOTAL_OUTS'

# predicted_probs = model.predict(...)

plot_woe_bin_target_rate('TOTAL_OUTS', woe_df, actual_df, predicted_probs)




def plot_feature_analysis(X, y, feature_name, nbins=10):
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import statsmodels.api as sm

    # Fit GEE model
    model = sm.GEE(y, X[[feature_name]], groups=X["MDM_ID"], 
                   cov_struct=sm.cov_struct.Independence(), 
                   family=sm.families.Binomial()).fit()
    pred_prob = model.predict(X[[feature_name]])

    # Create and sort dataframe
    df = pd.DataFrame({
        'Feature': X[feature_name],
        'Target': y,
        'Pred_Prob': pred_prob
    }).sort_values(by='Feature')

    # Bin settings
    spacing = round(len(df) / nbins)
    stop = spacing

    actual_rates = []
    pred_probs = []
    avg_x_bin = []

    # Binning loop
    start = 0
    for i in range(nbins):
        if i == nbins - 1:
            actual_rate = df['Target'][start:].mean()
            pred_prob_bin = df['Pred_Prob'][start:].mean()
            avg_x = df['Feature'][start:].mean()
        else:
            actual_rate = df['Target'][start:stop].mean()
            pred_prob_bin = df['Pred_Prob'][start:stop].mean()
            avg_x = df['Feature'][start:stop].mean()

        actual_rates.append(actual_rate)
        pred_probs.append(pred_prob_bin)
        avg_x_bin.append(avg_x)

        start = stop
        stop += spacing

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(avg_x_bin, actual_rates, marker='o', label='Actual Target Rate', color='blue')
    plt.plot(avg_x_bin, pred_probs, marker='x', label='Predicted Probability', color='goldenrod')
    plt.xlabel(feature_name)
    plt.ylabel('Target Rate / Predicted Probability')
    plt.title(f'Target Rate vs. {feature_name}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



Cramér’s V: Effect Size for Default Rate Differences

While the Chi-squared test determines whether a statistically significant relationship exists between segment membership (≤3MM vs >3MM) and default outcomes, it does not indicate the strength of that relationship. In large datasets, even small differences in default rates can produce significant p-values — especially when comparing segmentations with different numbers of groups (e.g., 2 vs 3), which results in different degrees of freedom. This makes raw test statistics not directly comparable between the proposed and legacy segmentation schemes.

To address this, we calculate Cramér’s V, a normalized measure of association strength for categorical variables. It ranges from 0 (no association) to 1 (perfect association), and is defined as:

V = \sqrt{\frac{\chi^2}{n \cdot (k - 1)}}

Where:
	•	\chi^2 = Chi-squared test statistic
	•	n = total number of observations
	•	k = the smaller of number of rows or columns in the contingency table

Interpretation Guidelines
	•	V < 0.10: Weak association
	•	0.10 \leq V < 0.30: Moderate association
	•	V \geq 0.30: Strong association



There is no statistically significant difference in default rates between the ≤3MM and >3MM commitment segments.

χ² = ∑ ((O_ij - E_ij)^2) / E_ij


## 4. Framework: Rolling Window Design

### Purpose

To ensure statistical validity and avoid overstating significance, we apply all segmentation tests using a **rolling window methodology**. This framework addresses the panel nature of the data and aligns analysis with changing macroeconomic cycles.

### Panel Structure and Motivation

The source data is panel-structured: each obligor has multiple time-indexed records. Conducting statistical tests on this directly would:
- Violate the assumption of independent observations
- Inflate sample size by duplicating obligors across time
- Introduce bias from events (e.g., default) that may occur long after the initial risk profile

### Design of the Rolling Window Framework

2.2 Design of the Rolling Window Framework

To ensure statistical rigor while maintaining temporal relevance, we implemented a rolling window framework. This approach addresses the panel nature of the data and avoids inflation of sample size by collapsing each obligor’s history into a single, time-aligned record per period.

Before finalizing the windows, we conducted an exploratory analysis of new default activity using a histogram of obligors newly defaulting each year. This allowed us to identify macroeconomic inflection points — such as spikes around the 2008 financial crisis or COVID-era instability — and group years into windows that align with distinct economic cycles.

We defined the following six rolling windows, each spanning 4 years:
	•	2003–2005: Pre-GFC expansion
	•	2006–2009: Global Financial Crisis
	•	2010–2013: Post-GFC recovery
	•	2014–2017: Stable low-rate environment
	•	2018–2021: Pandemic and stimulus period
	•	2022–2024: Inflationary tightening and normalization

Within each window:
	•	We extract the first observed record (T₀) per obligor
	•	We assess feature values at T₀ (e.g., risk rating, commitment, outstandings)
	•	We track default behavior within 12 months following T₀

This framework enables:
	•	Consistent alignment of features and outcomes
	•	One-time sampling per obligor per window
	•	Comparability of results across windows

By grounding the windows in empirical default behavior and aligning them with macroeconomic phases, this structure ensures that segmentation performance is evaluated not just in aggregate, but across varying credit environments.



We divide the full dataset (2003–2024) into consecutive **rolling windows**, each spanning four years:
- 2003–2005
- 2006–2009
- 2010–2013
- 2014–2017
- 2018–2021
- 2022–2024

These were chosen to **capture full economic cycles**, including pre-crisis, crisis, recovery, and post-pandemic periods.

Within each window:
- For each obligor, we identify the **first active snapshot** (T₀)
- Use that T₀ record to extract credit features
- Track whether the obligor **defaulted in the next 12 months** following T₀

This ensures:
- One record per obligor per window
- Alignment of features and outcomes
- Testing is repeated across different market conditions for stability

---

## 5. Method 1: Chi-Squared Test on Default Rates

### Objective

To test whether **default rates differ significantly** between segments (≤3MM vs >3MM), across time windows.

### Methodology

For each rolling window, we build a **2x2 contingency table** of default outcomes:

| Segment | Default (1) | No Default (0) |
|---------|-------------|----------------|
| ≤ 3MM   |      a      |       b        |
| > 3MM   |      c      |       d        |

We apply a **Chi-squared test of independence**:



χ² = ∑ ((Oᵢⱼ - Eᵢⱼ)² / Eᵢⱼ)

Where:
- *Oᵢⱼ* is the observed frequency
- *Eᵢⱼ* is the expected frequency under the assumption of independence

### Hypotheses

- **H₀ (Null):** Default rates are independent of commitment segment  
- **H₁ (Alt):** Default rates differ between commitment segments

### Pros
- Simple and interpretable
- Applicable to binary outcomes
- Useful for comparing performance across rolling windows

### Cons
- Sensitive to low cell counts (sparse defaults)
- Does not explain *why* default differs
- Requires binary outcome; no insight into underlying drivers

### [Insert Results Here]

---

## 6. Method 2: Mann–Whitney U Test on Credit Characteristics

### Objective

To test whether the **distributions of key credit features** differ between commitment segments — indicating structural differences in credit risk profiles.

### Variables Analyzed

- **Risk Rating:** Ordinal credit risk classification  
- **Outstanding Amount:** Continuous exposure measure

These variables reflect key risk characteristics available at origination and distinguish obligor financial scale and quality.

### Methodology

The **Mann–Whitney U Test** compares whether one group tends to have higher values than the other — without assuming normality.

U = min(U₁, U₂), where U₁ = R₁ - n₁(n₁ + 1) / 2

### Hypotheses (for each variable):

- **H₀:** Distribution of variable is the same across segments  
- **H₁:** Distributions differ significantly between segments

### Pros
- Non-parametric: handles skewed variables (e.g., outstandings)
- Appropriate for ordinal and continuous features
- Highlights structural credit differences

### Cons
- Only applicable to 2-group segmentation (not legacy)
- Sensitive to sample imbalance or ties
- Does not indicate effect size (though Cliff’s delta could be used)

### [Insert Results Here]

---

## 7. Method 3: PCA Decomposition for Structural Validation

### Objective

To visually and analytically assess whether commitment segments naturally form **distinct clusters** in multivariate credit space.

### Methodology

We apply **Principal Component Analysis (PCA)** to a set of scaled credit features:
- Risk Rating
- Outstanding Amount
- Commitment Amount
- (Optional: Interest Rate)

All features are standardized as:

Z = (x - μ) / σ

PCA projects the data into two orthogonal axes (PC1, PC2) that explain the **maximum variance**. We then visualize obligors in PC1–PC2 space, colored by:
- Commitment Segment
- Modeling Segment (for comparison)

### Interpretation

- **If commitment segments cluster separately**, it suggests:
  - Credit profiles differ structurally
  - Commitment segmentation aligns with latent credit behavior

- **If legacy segments show overlap**, it supports moving to a simpler, risk-aligned segmentation.

### Pros
- Reveals underlying structure of credit features
- Unsupervised (not dependent on outcome)
- Enables side-by-side visual comparison of segmentation schemes

### Cons
- PCA is linear and sensitive to outliers
- Visual inspection is subjective unless supplemented with metrics
- Segment overlap may reflect model limitations or true similarity

### [Insert Results Here]








Background and Motivation

The Commercial and Industrial (C&I) portfolio at [Your Bank Name] has traditionally been segmented into separate modeling populations based on business groupings such as Middle Market, Institutional Banking, and Leasing. Each of these segments historically had its own Probability of Default (PD) model, designed to reflect presumed differences in credit risk behavior across business lines.

However, this legacy segmentation strategy, while organizationally intuitive, has notable drawbacks:
	•	Segment boundaries are operational, not risk-based, and may group together obligors with heterogeneous risk profiles.
	•	Certain segments have become increasingly thin, especially in volatile periods, weakening model stability and leading to challenges in performance tracking and model governance.
	•	As obligor behavior shifts with market cycles and underwriting changes, static business-line definitions fail to reflect evolving credit dynamics.

⸻

Motivation for Exploring Commitment-Based Segmentation

To address these limitations, we evaluated an alternative approach: segmenting obligors based on their total commitment amount, using a cutoff of $3MM to define:
	•	≤ $3MM: Typically smaller businesses with more limited credit access, often exposed to localized economic trends
	•	> $3MM: Larger, more sophisticated firms with broader access to capital markets and more robust financial disclosures

This segmentation strategy is appealing for several reasons:
	•	Commitment is a consistent, objective, and readily available measure across all obligors.
	•	It introduces parsimonious segmentation (2 groups instead of 3–4), reducing model governance complexity.
	•	From a credit risk perspective, commitment size naturally proxies for borrower scale, financial health, and resource availability.
	•	Operationally, commitment-based segmentation aligns better with certain regulatory expectations for threshold-based risk differentiation.

⸻

Regulatory and Model Risk Considerations

Any change in segmentation methodology must be justified through a statistically sound framework that demonstrates:
	•	The new segmentation produces meaningfully different risk profiles
	•	It captures structural differences in credit behavior
	•	It performs at least as well as the legacy approach across economic cycles

To that end, we developed a three-phase statistical testing methodology applied across rolling economic windows, which is described in the following sections

Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature




Model Risk Report: Statistical Justification for Commitment-Based Segmentation

1. Executive Summary
	•	Purpose of segmentation
	•	Summary of methodology and findings
	•	Final recommendation

⸻

2. Background and Motivation
	•	Legacy segmentation strategy overview
	•	Rationale for exploring commitment-based segmentation
	•	Regulatory and business context

⸻

3. Data Overview
	•	Source of data and structure (panel nature, time span)
	•	Key fields used (commitment, defaults, credit features)
	•	Preprocessing and filtering rules
	•	Removal of invalid/negative commitments
	•	Handling of obligors with no defaults
	•	Rolling up to obligor level

⸻

4. Framework: Rolling Window Design
	•	Justification for rolling window
	•	Description of economic windows used
	•	Why fixed T₀ snapshots per window
	•	How windows align with macro trends
	•	Diagram of the pipeline (optional)

⸻

5. Method 1: Chi-Squared Test on Default Rates
	•	Purpose of the test
	•	Hypothesis formulation
	•	Construction of contingency table
	•	Assumptions and limitations
	•	Summary table of results by window
	•	Interpretation of test statistics
	•	Visualizations: bar plots / Cramér’s V

⸻

6. Method 2: Mann–Whitney U Test on Credit Drivers
	•	Rationale for non-parametric comparison
	•	Selection of variables (Risk Rating, Outstanding Amt, etc.)
	•	Hypothesis setup for each variable
	•	Segment-specific analysis (≤3MM vs >3MM)
	•	Summary of p-values across windows
	•	Limitations (e.g., cannot be used for >2 groups)
	•	Visual comparisons (boxplots or violin plots)

⸻

7. Method 3: PCA-Based Decomposition
	•	Purpose: Visual structural separation of segments
	•	Feature selection (scaled risk variables)
	•	Technical steps of PCA
	•	Explanation of principal components
	•	Plots: PCA by window (side-by-side for Commitment vs. Legacy)
	•	Interpretation of observed separation
	•	Handling outliers and overlap
	•	Summary of findings

⸻

8. Comparative Analysis
	•	Method-by-method comparison: commitment vs legacy
	•	Tradeoffs: parsimony vs. complexity
	•	Segment quality: size, default distribution, interpretability
	•	Model performance (optional if you ran PD models)

⸻

9. Conclusions and Recommendations
	•	Final summary of statistical evidence
	•	Does segmentation based on commitment make sense?
	•	When it works better (or not)
	•	Business and model development implications

⸻

10. Appendix
	•	Full test statistics and results
	•	Code snippets (optional)
	•	Cramér’s V formula
	•	Details on variable scaling
	•	Reference materials or literature
