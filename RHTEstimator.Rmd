no---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(plm)

# -----------------------------------------------------------
# 1) Read in your data from a .dat file
# -----------------------------------------------------------
# Example: data file "mydata.dat" with columns:
#   id, t, lwage, wks, union, exp, exp2, south, smsa, fem, blk, ed
# Adjust the path, separators, and other arguments as needed.
df_r <- read.table("/psidextract.dat", header = TRUE)

# -----------------------------------------------------------
# 2) Convert to a panel-data frame
# -----------------------------------------------------------
# index = c("id","t") indicates "id" is the individual, "t" is the time dimension
df_r <- plm.data(df_r, index = c("id", "t"))

You are a helpful data scientist tasked with creating a function that builds and visualizes a log of odds plot for a logistic regression analysis. The function should be capable of accepting input data, the target variable, and the name of the feature column for which the log of odds needs to be calculated. This task includes fitting the model, calculating probabilities, transforming these into log odds, and visualizing the results effectively.

Function Requirements:

Input Parameters:

The function should accept the predictor data X, the target variable y, and the name of the feature column feature_name.
Model Training:

Fit a logistic regression model to the data, ensuring to handle any class imbalances appropriately.
Probability and Log Odds Calculation:

Calculate the predicted probabilities for the positive class and transform both these probabilities and the actual target values into log odds.
Data Binning and Aggregation:

Sort and bin the data based on the feature values. Calculate the average of actual and predicted log odds for each bin.
Visualization:

Visualize the actual versus predicted log odds using a scatter plot for actual values and a line plot for predicted values on dual y-axes.
Handling Edge Cases:

Ensure the function can handle cases where probabilities are very close to 0 or 1, which can result in infinite log odds, by removing or adjusting these values.
Function Output:

The function should plot the relationship between the feature values and the log odds, providing a visual comparison of the model's predictions against the actual data, highlighting how well the model captures the underlying trend.



 
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GroupKFold, GridSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.metrics import roc_auc_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from statsmodels.api import Logit, add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Load Data
file_path = "your_data.csv"  # Change this to your CSV file path
df = pd.read_csv(file_path)

# Define target and features
target_col = "delq"  # Change to your actual target column name
group_col = "MDM_ID"  # Change to the obligor ID column
feature_cols = [col for col in df.columns if col not in [target_col, group_col]]

X = df[feature_cols]
y = df[target_col]
groups = df[group_col]  # Obligors for GroupKFold

# Handle missing values (fill with median)
X.fillna(X.median(), inplace=True)

# Standardize features (optional for RF but improves interpretability)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# ----- VIF Calculation -----
X_const = add_constant(X_scaled)
vif = pd.DataFrame()
vif["Feature"] = X_const.columns
vif["VIF"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]
vif = vif[vif["Feature"] != "const"]  # Exclude constant term
print("Variance Inflation Factor (VIF):")
print(vif)

# ----- GroupKFold for Cross Validation -----
gkf = GroupKFold(n_splits=5)

# Hyperparameter Grid
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 10, 15],
    'min_samples_split': [5, 10, 20],
    'min_samples_leaf': [2, 5, 10]
}

rf = RandomForestClassifier(random_state=42, class_weight="balanced")

grid_search = GridSearchCV(rf, param_grid, scoring="roc_auc", cv=gkf.split(X_scaled, y, groups), n_jobs=-1, verbose=1)
grid_search.fit(X_scaled, y)
best_rf = grid_search.best_estimator_

print(f"Best Parameters: {grid_search.best_params_}")

# ----- Evaluation -----
metrics = {}

for train_idx, test_idx in gkf.split(X_scaled, y, groups):
    X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # Over-Sampling
    smote = SMOTE(random_state=42)
    X_train_os, y_train_os = smote.fit_resample(X_train, y_train)

    # Fit Model
    best_rf.fit(X_train, y_train)
    best_rf_os = best_rf.fit(X_train_os, y_train_os)

    # Predictions & Probabilities
    y_pred_train = best_rf.predict(X_train)
    y_pred_test = best_rf.predict(X_test)
    y_pred_train_os = best_rf_os.predict(X_train_os)
    
    y_prob_train = best_rf.predict_proba(X_train)[:, 1]
    y_prob_test = best_rf.predict_proba(X_test)[:, 1]
    y_prob_train_os = best_rf_os.predict_proba(X_train_os)[:, 1]

    # Compute AUC Scores
    metrics["auc_train_os"] = roc_auc_score(y_train_os, y_prob_train_os)
    metrics["auc_train"] = roc_auc_score(y_train, y_prob_train)
    metrics["auc_test_os"] = roc_auc_score(y_test, y_prob_test)
    metrics["auc_test"] = roc_auc_score(y_test, y_prob_test)

    # Compute Precision & Recall
    metrics["precision_train_os"] = precision_score(y_train_os, y_pred_train_os)
    metrics["precision_train"] = precision_score(y_train, y_pred_train)
    metrics["precision_test_os"] = precision_score(y_test, y_pred_test)
    metrics["precision_test"] = precision_score(y_test, y_pred_test)

    metrics["recall_train_os"] = recall_score(y_train_os, y_pred_train_os)
    metrics["recall_train"] = recall_score(y_train, y_pred_train)
    metrics["recall_test_os"] = recall_score(y_test, y_pred_test)
    metrics["recall_test"] = recall_score(y_test, y_pred_test)

    # Compute AIC & BIC
    logit_model = Logit(y_train, add_constant(X_train)).fit(disp=0)
    metrics["AIC"] = logit_model.aic
    metrics["BIC"] = logit_model.bic

    # Compute Precision @ Top 1% & Top 5%
    def precision_at_top_k(y_true, y_prob, k):
        top_k = int(len(y_true) * k)
        sorted_indices = np.argsort(y_prob)[::-1]  # Sort by probability desc
        top_k_labels = y_true.iloc[sorted_indices][:top_k]
        return np.mean(top_k_labels)  # Precision = % of actual 1s in top K

    metrics["precision_top5"] = precision_at_top_k(y_test, y_prob_test, 0.05)
    metrics["precision_top1"] = precision_at_top_k(y_test, y_prob_test, 0.01)

    break  # Only compute on the first split

# Print Metrics
for metric, value in metrics.items():
    print(f"{metric}: {value:.#






    # Append results to list
    all_model_results.append(metrics)

# Convert results to DataFrame
results_df = pd.DataFrame(all_model_results)

# Save to CSV
results_df.to_csv(csv_filename, index=False)

print(f"Model results saved in {csv_filename}")


##

# Initialize a list to store results
all_model_results = []
model_counter = 1

# Loop through all hyperparameter combinations
for params in param_combinations:
    n_estimators, max_depth, min_samples_split, min_samples_leaf = params

    # Define model with current hyperparameters
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42,
        class_weight="balanced"
    )

    model_name = f"candidate_model_{model_counter}_{n_estimators}_{max_depth}_{min_samples_split}_{min_samples_leaf}"
    model_counter += 1  # Increment model number

    metrics = {"Model Name": model_name}

    # Initialize lists to store CV results
    auc_train_list, auc_test_list = [], []
    precision_train_list, precision_test_list = [], []
    recall_train_list, recall_test_list = [], []
    aic_list, bic_list = [], []
    precision_top5_list, precision_top1_list = [], []

    # Run cross-validation over all splits
    for train_idx, test_idx in gkf.split(X_scaled, y, groups):
        X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply Over-Sampling using SMOTE
        smote = SMOTE(random_state=42)
        X_train_os, y_train_os = smote.fit_resample(X_train, y_train)

        # Fit Model (Cross-Validation Step)
        rf.fit(X_train, y_train)

        # Predictions & Probabilities
        y_prob_train = rf.predict_proba(X_train)[:, 1]
        y_prob_test = rf.predict_proba(X_test)[:, 1]

        # Compute Metrics for Each Fold
        auc_train_list.append(roc_auc_score(y_train, y_prob_train))
        auc_test_list.append(roc_auc_score(y_test, y_prob_test))

        precision_train_list.append(precision_score(y_train, rf.predict(X_train)))
        precision_test_list.append(precision_score(y_test, rf.predict(X_test)))

        recall_train_list.append(recall_score(y_train, rf.predict(X_train)))
        recall_test_list.append(recall_score(y_test, rf.predict(X_test)))

        # Compute AIC & BIC for Each Fold
        logit_model = Logit(y_train, add_constant(X_train)).fit(disp=0)
        aic_list.append(logit_model.aic)
        bic_list.append(logit_model.bic)

        # Compute Precision @ Top 1% & Top 5%
        def precision_at_top_k(y_true, y_prob, k):
            top_k = int(len(y_true) * k)
            sorted_indices = np.argsort(y_prob)[::-1]  # Sort by probability desc
            top_k_labels = y_true.iloc[sorted_indices][:top_k]
            return np.mean(top_k_labels)  # Precision = % of actual 1s in top K

        precision_top5_list.append(precision_at_top_k(y_test, y_prob_test, 0.05))
        precision_top1_list.append(precision_at_top_k(y_test, y_prob_test, 0.01))

    # Store the averaged cross-validated metrics
    metrics["auc_train"] = np.mean(auc_train_list)
    metrics["auc_test"] = np.mean(auc_test_list)
    metrics["precision_train"] = np.mean(precision_train_list)
    metrics["precision_test"] = np.mean(precision_test_list)
    metrics["recall_train"] = np.mean(recall_train_list)
    metrics["recall_test"] = np.mean(recall_test_list)
    metrics["AIC"] = np.mean(aic_list)
    metrics["BIC"] = np.mean(bic_list)
    metrics["precision_top5"] = np.mean(precision_top5_list)
    metrics["precision_top1"] = np.mean(precision_top1_list)

    # Append model results
    all_model_results.append(metrics)

# Convert results to DataFrame and save to CSV
results_df = pd.DataFrame(all_model_results)
results_df.to_csv(csv_filename, index=False)

print(f"Model results saved in {csv_filename}")
