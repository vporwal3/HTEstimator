no---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(plm)

# -----------------------------------------------------------
# 1) Read in your data from a .dat file
# -----------------------------------------------------------
# Example: data file "mydata.dat" with columns:
#   id, t, lwage, wks, union, exp, exp2, south, smsa, fem, blk, ed
# Adjust the path, separators, and other arguments as needed.
df_r <- read.table("/psidextract.dat", header = TRUE)

# -----------------------------------------------------------
# 2) Convert to a panel-data frame
# -----------------------------------------------------------
# index = c("id","t") indicates "id" is the individual, "t" is the time dimension
df_r <- plm.data(df_r, index = c("id", "t"))

You are a helpful data scientist tasked with creating a function that builds and visualizes a log of odds plot for a logistic regression analysis. The function should be capable of accepting input data, the target variable, and the name of the feature column for which the log of odds needs to be calculated. This task includes fitting the model, calculating probabilities, transforming these into log odds, and visualizing the results effectively.

Function Requirements:

Input Parameters:

The function should accept the predictor data X, the target variable y, and the name of the feature column feature_name.
Model Training:

Fit a logistic regression model to the data, ensuring to handle any class imbalances appropriately.
Probability and Log Odds Calculation:

Calculate the predicted probabilities for the positive class and transform both these probabilities and the actual target values into log odds.
Data Binning and Aggregation:

Sort and bin the data based on the feature values. Calculate the average of actual and predicted log odds for each bin.
Visualization:

Visualize the actual versus predicted log odds using a scatter plot for actual values and a line plot for predicted values on dual y-axes.
Handling Edge Cases:

Ensure the function can handle cases where probabilities are very close to 0 or 1, which can result in infinite log odds, by removing or adjusting these values.
Function Output:

The function should plot the relationship between the feature values and the log odds, providing a visual comparison of the model's predictions against the actual data, highlighting how well the model captures the underlying 


import pandas as pd
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
from monotonic_binning.monotonic_woe_binning import Binning

base = pd.read_csv(model_file)

def process_variable(x_var, base):
    y_var = 'TARGET'  # Binary target for delinquency

    bin_object = Binning(y_var, n_threshold=10, y_threshold=2, 
                         p_threshold=0.02, max_n_bins=10, sign=False)
    
    try:
        bin_object.fit(base[[y_var, x_var]])
        df_woe = bin_object.dataset
        df_woe = df_woe.sort_values(x_var)
        idx = df_woe.index
        
        # Get WoE values
        mapping = bin_object.woe_summary[[x_var, 'WoE_' + x_var]].sort_values(x_var, ignore_index=True)
        df_woe = pd.merge_asof(df_woe, mapping, on=x_var, direction='forward')
        df_woe.index = idx
        df_woe = df_woe.sort_index()

        # Debugging: Check number of bins
        print(f"Variable: {x_var}")
        print(f"Unique WoE values: {df_woe['WoE_' + x_var].nunique()}")
        print(f"Bin counts: \n{df_woe['bins'].value_counts()}")

        # If the variable collapsed into a single bin, check IV
        if df_woe['WoE_' + x_var].nunique() == 1:
            print(f"{x_var} collapsed into 1 bin. Possible low IV: {bin_object.information_value}")

        # Keep variables that are meaningful (not collapsed to 1 bin)
        if not all(df_woe['WoE_' + x_var] == 0):
            return df_woe['WoE_' + x_var], x_var

        return None, None
    except Exception as e:
        print(f"Skipping {x_var} due to error: {e}")
        return None, None

# Parallel Processing
woe_vars = []
non_woe_vars = []
num_cores = multiprocessing.cpu_count()

with ThreadPoolExecutor(max_workers=num_cores) as executor:
    futures = {executor.submit(process_variable, x_var, base): x_var for x_var in ret_vars}
    
    for future in futures:
        try:
            woe_series, x_var = future.result()
            if woe_series is not None:
                base['WoE_' + x_var] = woe_series
                woe_vars.append(x_var)
            else:
                non_woe_vars.append(x_var)
        except Exception as e:
            print(f"Thread error: {e}")






# ----- Evaluation -----
metrics = {}

for train_idx, test_idx in gkf.split(X_scaled, y, groups):
    X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # Over-Sampling
    smote = SMOTE(random_state=42)
    X_train_os, y_train_os = smote.fit_resample(X_train, y_train)

    # Fit Model
    best_rf.fit(X_train, y_train)
    best_rf_os = best_rf.fit(X_train_os, y_train_os)

    # Predictions & Probabilities
    y_pred_train = best_rf.predict(X_train)
    y_pred_test = best_rf.predict(X_test)
    y_pred_train_os = best_rf_os.predict(X_train_os)
    
    y_prob_train = best_rf.predict_proba(X_train)[:, 1]
    y_prob_test = best_rf.predict_proba(X_test)[:, 1]
    y_prob_train_os = best_rf_os.predict_proba(X_train_os)[:, 1]

    # Compute AUC Scores
    metrics["auc_train_os"] = roc_auc_score(y_train_os, y_prob_train_os)
    metrics["auc_train"] = roc_auc_score(y_train, y_prob_train)
    metrics["auc_test_os"] = roc_auc_score(y_test, y_prob_test)
    metrics["auc_test"] = roc_auc_score(y_test, y_prob_test)

    # Compute Precision & Recall
    metrics["precision_train_os"] = precision_score(y_train_os, y_pred_train_os)
    metrics["precision_train"] = precision_score(y_train, y_pred_train)
    metrics["precision_test_os"] = precision_score(y_test, y_pred_test)
    metrics["precision_test"] = precision_score(y_test, y_pred_test)

    metrics["recall_train_os"] = recall_score(y_train_os, y_pred_train_os)
    metrics["recall_train"] = recall_score(y_train, y_pred_train)
    metrics["recall_test_os"] = recall_score(y_test, y_pred_test)
    metrics["recall_test"] = recall_score(y_test, y_pred_test)

    # Compute AIC & BIC
    logit_model = Logit(y_train, add_constant(X_train)).fit(disp=0)
    metrics["AIC"] = logit_model.aic
    metrics["BIC"] = logit_model.bic

    # Compute Precision @ Top 1% & Top 5%
    def precision_at_top_k(y_true, y_prob, k):
        top_k = int(len(y_true) * k)
        sorted_indices = np.argsort(y_prob)[::-1]  # Sort by probability desc
        top_k_labels = y_true.iloc[sorted_indices][:top_k]
        return np.mean(top_k_labels)  # Precision = % of actual 1s in top K

    metrics["precision_top5"] = precision_at_top_k(y_test, y_prob_test, 0.05)
    metrics["precision_top1"] = precision_at_top_k(y_test, y_prob_test, 0.01)

    break  # Only compute on the first split

# Print Metrics
for metric, value in metrics.items():
    print(f"{metric}: {value:.#






    # Append results to list
    all_model_results.append(metrics)

# Convert results to DataFrame
results_df = pd.DataFrame(all_model_results)

# Save to CSV
results_df.to_csv(csv_filename, index=False)

print(f"Model results saved in {csv_filename}")


##

import os
import datetime
import pandas as pd
import numpy as np
from itertools import product
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GroupKFold
from imblearn.over_sampling import SMOTE
from sklearn.metrics import roc_auc_score, precision_score, recall_score
from statsmodels.api import Logit, add_constant

# Define hyperparameter grid manually
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 10, 15],
    'min_samples_split': [5, 10, 20],
    'min_samples_leaf': [2, 5, 10]
}

# Generate all possible hyperparameter combinations
param_combinations = list(product(
    param_grid['n_estimators'],
    param_grid['max_depth'],
    param_grid['min_samples_split'],
    param_grid['min_samples_leaf']
))

# Create timestamped folder for results
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
results_dir = f"run-{timestamp}"
os.makedirs(results_dir, exist_ok=True)

# Define CSV file to store model metrics
csv_filename = os.path.join(results_dir, "model_metrics.csv")

# Initialize a list to store results
all_model_results = []

# Set up GroupKFold
gkf = GroupKFold(n_splits=5)



import os
import datetime
import numpy as np
import pandas as pd
from itertools import product
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GroupKFold
from imblearn.over_sampling import SMOTE
from sklearn.metrics import roc_auc_score, precision_score, recall_score
from statsmodels.api import Logit, add_constant

# Define hyperparameter grid manually
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 10, 15],
    'min_samples_split': [5, 10, 20],
    'min_samples_leaf': [2, 5, 10]
}

# Generate all possible hyperparameter combinations
param_combinations = list(product(
    param_grid['n_estimators'],
    param_grid['max_depth'],
    param_grid['min_samples_split'],
    param_grid['min_samples_leaf']
))

# Create timestamped folder for results
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
results_dir = f"run-{timestamp}"
os.makedirs(results_dir, exist_ok=True)

# Define CSV file to store model metrics
csv_filename = os.path.join(results_dir, "model_metrics.csv")

# Initialize a list to store results
all_model_results = []
model_counter = 1

# Set up GroupKFold
gkf = GroupKFold(n_splits=5)

# Loop through all hyperparameter combinations
for params in param_combinations:
    n_estimators, max_depth, min_samples_split, min_samples_leaf = params

    # Define model with current hyperparameters
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42,
        class_weight="balanced"
    )

    model_name = f"candidate_model_{model_counter}_{n_estimators}_{max_depth}_{min_samples_split}_{min_samples_leaf}"
    model_counter += 1  # Increment model number

    metrics = {"Model Name": model_name}

    # Initialize lists to store CV results
    auc_train_list, auc_test_list = [], []
    auc_train_os_list, auc_test_os_list = [], []  # Oversampled AUC
    precision_train_list, precision_test_list = [], []
    precision_train_os_list, precision_test_os_list = [], []  # Oversampled Precision
    recall_train_list, recall_test_list = [], []
    recall_train_os_list, recall_test_os_list = [], []  # Oversampled Recall
    aic_list, bic_list = [], []
    precision_top5_list, precision_top1_list = [], []
    precision_top5_os_list, precision_top1_os_list = [], []  # Oversampled Precision at Top %

    # Run cross-validation over all splits
    for train_idx, test_idx in gkf.split(X_scaled, y, groups):
        X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply Over-Sampling using SMOTE
        smote = SMOTE(random_state=42)
        X_train_os, y_train_os = smote.fit_resample(X_train, y_train)

        # Fit Model on Original Data
        rf.fit(X_train, y_train)
        y_prob_train = rf.predict_proba(X_train)[:, 1]
        y_prob_test = rf.predict_proba(X_test)[:, 1]

        # Fit Model on Oversampled Data
        rf_os = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            random_state=42,
            class_weight="balanced"
        )
        rf_os.fit(X_train_os, y_train_os)
        y_prob_train_os = rf_os.predict_proba(X_train_os)[:, 1]
        y_prob_test_os = rf_os.predict_proba(X_test)[:, 1]

        # Compute Metrics for Each Fold (Original)
        auc_train_list.append(roc_auc_score(y_train, y_prob_train))
        auc_test_list.append(roc_auc_score(y_test, y_prob_test))

        precision_train_list.append(precision_score(y_train, rf.predict(X_train)))
        precision_test_list.append(precision_score(y_test, rf.predict(X_test)))

        recall_train_list.append(recall_score(y_train, rf.predict(X_train)))
        recall_test_list.append(recall_score(y_test, rf.predict(X_test)))

        # Compute Metrics for Each Fold (Oversampled)
        auc_train_os_list.append(roc_auc_score(y_train_os, y_prob_train_os))
        auc_test_os_list.append(roc_auc_score(y_test, y_prob_test_os))

        precision_train_os_list.append(precision_score(y_train_os, rf_os.predict(X_train_os)))
        precision_test_os_list.append(precision_score(y_test, rf_os.predict(X_test)))

        recall_train_os_list.append(recall_score(y_train_os, rf_os.predict(X_train_os)))
        recall_test_os_list.append(recall_score(y_test, rf_os.predict(X_test)))

        # Compute AIC & BIC for Each Fold (Using Logistic Regression for Comparison)
        logit_model = Logit(y_train, add_constant(X_train)).fit(disp=0)
        aic_list.append(logit_model.aic)
        bic_list.append(logit_model.bic)

        # Compute Precision @ Top 1% & Top 5% (Original)
        def precision_at_top_k(y_true, y_prob, k):
            top_k = int(len(y_true) * k)
            sorted_indices = np.argsort(y_prob)[::-1]  # Sort by probability desc
            top_k_labels = y_true.iloc[sorted_indices][:top_k]
            return np.mean(top_k_labels)  # Precision = % of actual 1s in top K

        precision_top5_list.append(precision_at_top_k(y_test, y_prob_test, 0.05))
        precision_top1_list.append(precision_at_top_k(y_test, y_prob_test, 0.01))

        # Compute Precision @ Top 1% & Top 5% (Oversampled)
        precision_top5_os_list.append(precision_at_top_k(y_test, y_prob_test_os, 0.05))
        precision_top1_os_list.append(precision_at_top_k(y_test, y_prob_test_os, 0.01))

    # Store the averaged cross-validated metrics
    metrics["auc_train"] = np.mean(auc_train_list)
    metrics["auc_test"] = np.mean(auc_test_list)
    metrics["auc_train_os"] = np.mean(auc_train_os_list)
    metrics["auc_test_os"] = np.mean(auc_test_os_list)

    metrics["precision_train"] = np.mean(precision_train_list)
    metrics["precision_test"] = np.mean(precision_test_list)
    metrics["precision_train_os"] = np.mean(precision_train_os_list)
    metrics["precision_test_os"] = np.mean(precision_test_os_list)

    metrics["recall_train"] = np.mean(recall_train_list)
    metrics["recall_test"] = np.mean(recall_test_list)
    metrics["recall_train_os"] = np.mean(recall_train​⬤



# Loop through all hyperparameter combinations
for params in param_combinations:
    n_estimators, max_depth, min_samples_split, min_samples_leaf = params

    # Define model with current hyperparameters
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42,
        class_weight="balanced"
    )

    model_name = f"candidate_model_{model_counter}_{n_estimators}_{max_depth}_{min_samples_split}_{min_samples_leaf}"
    model_counter += 1  # Increment model number

    metrics = {"Model Name": model_name}

    # Initialize lists to store CV results
    auc_train_list, auc_test_list, auc_train_os_list, auc_test_os_list = [], [], [], []
    precision_train_list, precision_test_list, precision_train_os_list, precision_test_os_list = [], [], [], []
    recall_train_list, recall_test_list, recall_train_os_list, recall_test_os_list = [], [], [], []
    aic_list, bic_list = [], []
    precision_top5_list, precision_top1_list, precision_top5_os_list, precision_top1_os_list = [], [], [], []

    # Run cross-validation over all splits
    for train_idx, test_idx in gkf.split(X_scaled, y, groups):
        X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Apply Over-Sampling using SMOTE
        smote = SMOTE(random_state=42)
        X_train_os, y_train_os = smote.fit_resample(X_train, y_train)

        # Fit Model on Original Data
        rf.fit(X_train, y_train)
        y_prob_train = rf.predict_proba(X_train)[:, 1]
        y_prob_test = rf.predict_proba(X_test)[:, 1]

        # Fit Model on Oversampled Data (Reusing `rf`)
        rf.fit(X_train_os, y_train_os)
        y_prob_train_os = rf.predict_proba(X_train_os)[:, 1]
        y_prob_test_os = rf.predict_proba(X_test)[:, 1]

        # Compute Metrics for Each Fold (Original)
        auc_train_list.append(roc_auc_score(y_train, y_prob_train))
        auc_test_list.append(roc_auc_score(y_test, y_prob_test))
        precision_train_list.append(precision_score(y_train, rf.predict(X_train)))
        precision_test_list.append(precision_score(y_test, rf.predict(X_test)))
        recall_train_list.append(recall_score(y_train, rf.predict(X_train)))
        recall_test_list.append(recall_score(y_test, rf.predict(X_test)))

        # Compute Metrics for Each Fold (Oversampled)
        auc_train_os_list.append(roc_auc_score(y_train_os, y_prob_train_os))
        auc_test_os_list.append(roc_auc_score(y_test, y_prob_test_os))
        precision_train_os_list.append(precision_score(y_train_os, rf.predict(X_train_os)))
        precision_test_os_list.append(precision_score(y_test, rf.predict(X_test)))
        recall_train_os_list.append(recall_score(y_train_os, rf.predict(X_train_os)))
        recall_test_os_list.append(recall_score(y_test, rf.predict(X_test)))

        # Compute AIC & BIC for Each Fold (Using Logistic Regression for Comparison)
        logit_model = Logit(y_train, add_constant(X_train)).fit(disp=0)
        aic_list.append(logit_model.aic)
        bic_list.append(logit_model.bic)

        # Compute Precision @ Top 1% & Top 5% (Original)
        def precision_at_top_k(y_true, y_prob, k):
            top_k = int(len(y_true) * k)
            sorted_indices = np.argsort(y_prob)[::-1]  # Sort by probability desc
            top_k_labels = y_true.iloc[sorted_indices][:top_k]
            return np.mean(top_k_labels)  # Precision = % of actual 1s in top K

        precision_top5_list.append(precision_at_top_k(y_test, y_prob_test, 0.05))
        precision_top1_list.append(precision_at_top_k(y_test, y_prob_test, 0.01))
        precision_top5_os_list.append(precision_at_top_k(y_test, y_prob_test_os, 0.05))
        precision_top1_os_list.append(precision_at_top_k(y_test, y_prob_test_os, 0.01))

    # Store the averaged cross-validated metrics
    metrics["auc_train"] = np.mean(auc_train_list)
    metrics["auc_test"] = np.mean(auc_test_list)
    metrics["auc_train_os"] = np.mean(auc_train_os_list)
    metrics["auc_test_os"] = np.mean(auc_test_os_list)
    metrics["precision_train"] = np.mean(precision_train_list)
    metrics["precision_test"] = np.mean(precision_test_list)
    metrics["precision_train_os"] = np.mean(precision_train_os_list)
    metrics["precision_test_os"] = np.mean(precision_test_os_list)
    metrics["recall_train"] = np.mean(recall_train_list)
    metrics["recall_test"] = np.mean(recall_test_list)
    metrics["recall_train_os"] = np.mean(recall_train_os_list)
    metrics["recall_test_os"] = np.mean(recall_test_os_list)
    metrics["AIC"] = np.mean(aic_list)
    metrics["BIC"] = np.mean(bic_list)
    metrics["precision_top5"] = np.mean(precision_top5_list)
    metrics["precision_top1"] = np.mean(precision_top1_list)
    metrics["precision_top5_os"] = np.mean(precision_top5_os_list)
    metrics["precision_top1_os"] = np.mean(precision_top1_os_list)

    # Append model results
    all_model_results.append(metrics)

# Convert results to DataFrame and save to CSV
results_df = pd.DataFrame(all_model_results)
results_df.to_csv(csv_filename, index=False)

print(f"Model results saved in {csv_filename}")



